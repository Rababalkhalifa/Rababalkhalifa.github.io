---
title: "LongEval: Longitudinal Evaluation of Model Performance at CLEF 2023"
collection: publications
permalink: /publication/2023-03-16-Longitudinal-Evaluation-of-Model-Performance-at-CLEF-2023-7
excerpt: "In this paper, we describe the plans for the first LongEval CLEF 2023 shared task dedicated to evaluating the temporal persistence of Information Retrieval (IR) systems and Text Classifiers. The task is motivated by recent research showing that the performance of these models drops as the test data becomes more distant, with respect to time, from the training data. LongEval differs from traditional shared IR and classification tasks by giving special consideration to evaluating models aiming to mitigate performance drop over time. We envisage that this task will draw attention from the IR community and NLP researchers to the problem of temporal persistence of models, what enables or prevents it, potential solutions and their limitations."
date: 2023-03-16
venue: "Springer Nature Switzerland"
paperurl: "https://link.springer.com/chapter/10.1007/978-3-031-28241-6_58"
citation: "Alkhalifa, R., Bilal, I., Borkakoty, H., Camacho-Collados, J., Deveaud, R., El-Ebshihy, A., ... & Zubiaga, A. (2023, March). LongEval: Longitudinal Evaluation of Model Performance at CLEF 2023. In Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part III (pp. 499-505). Cham: Springer Nature Switzerland."
---
In this paper, we describe the plans for the first LongEval CLEF 2023 shared task dedicated to evaluating the temporal persistence of Information Retrieval (IR) systems and Text Classifiers. The task is motivated by recent research showing that the performance of these models drops as the test data becomes more distant, with respect to time, from the training data. LongEval differs from traditional shared IR and classification tasks by giving special consideration to evaluating models aiming to mitigate performance drop over time. We envisage that this task will draw attention from the IR community and NLP researchers to the problem of temporal persistence of models, what enables or prevents it, potential solutions and their limitations.

[Download paper here](https://link.springer.com/chapter/10.1007/978-3-031-28241-6_58)